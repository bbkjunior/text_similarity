{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тематическое моделирование\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем топ-10 тематических предложений для каждого абзаца\n",
    "\n",
    "Находим пересечение сетов полученных тематических предложений друг с другом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora, models\n",
    "\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(file):\n",
    "    \"\"\"Получаем на вход файл и отдаем лист листов из токенов\"\"\"\n",
    "    tokenized_text = []\n",
    "    with open (file, \"r\", encoding = \"utf-8\") as file:\n",
    "        for line in file:\n",
    "            tokenized_line = line.split(' ')\n",
    "            tokenized_line = [tok.replace('\\n', '')  for tok in tokenized_line if (len(tok) > 2)]\n",
    "            if(tokenized_line):\n",
    "                tokenized_text.append(tokenized_line)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_rzn = tokenize_file(\"speech_rzn_processed.txt\")\n",
    "tokenized_tmn = tokenize_file(\"speech_tmn_processed.txt\")\n",
    "tokenized_tula = tokenize_file(\"speech_tula_processed.txt\")\n",
    "tokenized_yml = tokenize_file(\"speech_yml_processed.txt\")\n",
    "tokenized_putin = tokenize_file(\"speech_putin_processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2018',\n",
       " 'возможность',\n",
       " 'время',\n",
       " 'выполнять',\n",
       " 'город',\n",
       " 'городской',\n",
       " 'государственный',\n",
       " 'гражданин',\n",
       " 'должный',\n",
       " 'дорога',\n",
       " 'жизнь',\n",
       " 'житель',\n",
       " 'задача',\n",
       " 'идти',\n",
       " 'использовать',\n",
       " 'каждый',\n",
       " 'качество',\n",
       " 'крупный',\n",
       " 'место',\n",
       " 'млн',\n",
       " 'млрд',\n",
       " 'муниципальный',\n",
       " 'направлять',\n",
       " 'начало',\n",
       " 'начинать',\n",
       " 'необходимо',\n",
       " 'областной',\n",
       " 'образование',\n",
       " 'общественный',\n",
       " 'объект',\n",
       " 'объем',\n",
       " 'первый',\n",
       " 'план',\n",
       " 'планировать',\n",
       " 'повышение',\n",
       " 'поддержка',\n",
       " 'позволять',\n",
       " 'помощь',\n",
       " 'правительство',\n",
       " 'предприятие',\n",
       " 'президент',\n",
       " 'проблема',\n",
       " 'продолжать',\n",
       " 'развивать',\n",
       " 'ребенок',\n",
       " 'результат',\n",
       " 'решение',\n",
       " 'россия',\n",
       " 'рост',\n",
       " 'самый',\n",
       " 'свой',\n",
       " 'система',\n",
       " 'следующий',\n",
       " 'создавать',\n",
       " 'создание',\n",
       " 'социальный',\n",
       " 'становиться',\n",
       " 'строительство',\n",
       " 'сфера',\n",
       " 'труд',\n",
       " 'улучшение',\n",
       " 'уровень',\n",
       " 'условие',\n",
       " 'услуга',\n",
       " 'федеральный',\n",
       " 'хотеть',\n",
       " 'центр',\n",
       " 'человек',\n",
       " 'число'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_topics_tfidf(tokenized_text):\n",
    "    \"\"\"производим моделирование 10 тем, возвращаем сет из уникальных слов в созданных темах\"\"\"\n",
    "    dictionary = Dictionary(tokenized_text)\n",
    "    dictionary.filter_extremes(no_above=0.1)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_rzn]\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    corpus_tfidf = tfidf[bow_corpus]\n",
    "    lda_model_tfidf = LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)     \n",
    "    topic_words_list = []\n",
    "    x=lda_model_tfidf.show_topics(num_topics=10,formatted=False)\n",
    "    topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "    for topic,words in topics_words:\n",
    "        topic_words_list += words    \n",
    "    return set(topic_words_list)\n",
    "    \n",
    "get_topics_tfidf(tokenized_tula)#проверяем пример вывода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Попарно без повторений сравниваем сеты слов из топ-10 тем каждого текста. Полчаем выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ryazan vs tyumen\n",
      "Topics intersection is {'оставаться', 'система', 'житель', 'общественный', 'хороший', 'самый', 'результат', 'первый', 'центр'}\n",
      "intersection size/intial set size=  0.12\n",
      "ryazan vs tula\n",
      "Topics intersection is {'труд', 'жизнь', 'общественный', 'место', 'центр', 'областной', 'система', 'житель', 'создавать', 'позволять', 'должный', 'повышение', 'программа', 'результат', 'объект', 'задача', 'качество', 'становиться', 'город', 'продолжать', 'нужно', 'ребенок', 'время', 'предприятие', 'инфраструктура', 'сфера', 'хотеть', 'уровень', 'первый'}\n",
      "intersection size/intial set size=  0.38666666666666666\n",
      "ryazan vs yamal\n",
      "Topics intersection is {'президент', 'направление', 'контроль', 'значимый', 'федеральный', 'инвестиционный', 'хороший', 'объем', 'реализация', 'построить', 'центр', 'организация', 'оставаться', 'рубль', 'являться', 'компания', 'повышение', 'программа', 'участие', 'объект', 'счет', 'задача', 'качество', 'город', 'внимание', 'технология', 'продолжать', 'ребенок', 'время', 'предприятие', 'сфера', 'уровень', 'первый'}\n",
      "intersection size/intial set size=  0.44\n",
      "ryazan vs putin\n",
      "Topics intersection is {'труд', 'жизнь', 'хороший', 'объем', 'место', 'центр', 'рубль', 'житель', 'цифровой', 'являться', 'компания', 'повышение', 'результат', 'счет', 'качество', 'принимать', 'становиться', 'город', 'внимание', 'технология', 'комплекс', 'подход', 'продолжать', 'также', '2017', 'ребенок', 'время', 'хотеть', 'перспективный', 'школа', 'первый'}\n",
      "intersection size/intial set size=  0.41333333333333333\n",
      "\n",
      "tyumen vs tula\n",
      "Topics intersection is {'система', 'главный', 'житель', 'общественный', 'поддержка', 'крупный', 'региональный', 'национальный', 'решение', 'отрасль', 'результат', 'первый', 'центр'}\n",
      "intersection size/intial set size=  0.17333333333333334\n",
      "tyumen vs yamal\n",
      "Topics intersection is {'рост', 'оставаться', 'первый', 'крупный', 'экономический', 'хороший', 'давать', 'решение', 'страна', 'центр'}\n",
      "intersection size/intial set size=  0.13333333333333333\n",
      "tyumen vs putin\n",
      "Topics intersection is {'житель', 'поддержка', 'хороший', 'национальный', 'крупный', 'результат', 'первый', 'центр'}\n",
      "intersection size/intial set size=  0.10666666666666667\n",
      "\n",
      "tula vs yamal\n",
      "Topics intersection is {'дорога', 'возможность', 'строительство', 'муниципальный', '2018', 'получать', 'центр', 'свой', 'повышение', 'программа', 'создание', 'объект', 'задача', 'качество', 'город', 'решение', 'крупный', 'человек', 'продолжать', 'ребенок', 'условие', 'начинать', 'время', 'предприятие', 'сфера', 'уровень', 'первый'}\n",
      "intersection size/intial set size=  0.36\n",
      "tula vs putin\n",
      "Topics intersection is {'труд', 'жизнь', 'использовать', 'дорога', 'строительство', '2018', 'необходимо', 'место', 'центр', 'житель', 'повышение', 'направлять', 'результат', 'создание', 'услуга', 'привлекать', 'качество', 'становиться', 'поддержка', 'город', 'крупный', 'продолжать', 'ребенок', 'условие', 'начинать', 'время', 'национальный', 'хотеть', 'тысяча', 'первый', 'число'}\n",
      "intersection size/intial set size=  0.41333333333333333\n",
      "\n",
      "yamal vs putin\n",
      "Topics intersection is {'дорога', 'строительство', 'хороший', 'объем', 'сохранять', '2018', 'область', 'центр', 'рубль', 'промышленный', 'компания', 'являться', 'повышение', 'создание', 'счет', 'качество', 'уважаемый', 'интерес', 'город', 'внимание', 'крупный', 'технология', 'продолжать', 'ребенок', 'условие', 'начинать', 'время', 'образование', 'первый'}\n",
      "intersection size/intial set size=  0.38666666666666666\n",
      "\n",
      "\n",
      "{'ryazan': 0.41333333333333333, 'tyumen': 0.10666666666666667, 'tula': 0.41333333333333333, 'yamal': 0.38666666666666666}\n",
      "tula is most similar to Putin speech\n"
     ]
    }
   ],
   "source": [
    "text_dict = {'ryazan':tokenized_rzn,\"tyumen\":tokenized_tmn,\"tula\": tokenized_tula, \n",
    "             \"yamal\": tokenized_yml, \"putin\":tokenized_putin}\n",
    "set_dict = {'ryazan':set(),\"tyumen\":set(),\"tula\": set(), \"yamal\": set(), \"putin\":set()}\n",
    "#подсчитываем сеты слов из тем всех данных текстов\n",
    "for key, value in text_dict.items():\n",
    "    set_dict[key] = get_topics_tfidf(text_dict[key])\n",
    "#создаем упорядоченный словарь, чтобы итерироваться по индексам во избежании повторных сравнений\n",
    "putin_similarity_dict = {'ryazan':0,\"tyumen\":0,\"tula\": 0, \"yamal\": 0}\n",
    "ordered_set_dict = OrderedDict(set_dict)\n",
    "keys = list(ordered_set_dict.keys())\n",
    "values = list(ordered_set_dict.values())\n",
    "#производим сравнение и выводим результаты\n",
    "for dict_ind in range(len(keys)):\n",
    "    current_topic_set = values[dict_ind]\n",
    "    for compare_ind in range(dict_ind + 1,len(keys)):\n",
    "        print(keys[dict_ind], \"vs\", keys[compare_ind])\n",
    "        compared_topic_set = values[compare_ind]\n",
    "        inersection = current_topic_set.intersection(compared_topic_set)\n",
    "        print(\"Topics intersection is\", inersection)\n",
    "        if(keys[dict_ind] != 'putin'):\n",
    "            length_initial = len(set_dict[key])\n",
    "            similarity = len(inersection)/length_initial\n",
    "            print(\"intersection size/intial set size= \", similarity)\n",
    "            putin_similarity_dict[keys[dict_ind]] = similarity\n",
    "    print( )\n",
    "print(putin_similarity_dict)\n",
    "sorted_x = sorted(putin_similarity_dict.items(), key=operator.itemgetter(1))\n",
    "print(sorted_x[-1][0],\"is most similar to Putin speech\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
